{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f347b00",
   "metadata": {},
   "source": [
    "# Depresjon - basic feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b2a9c9",
   "metadata": {},
   "source": [
    "This notebook aims to recreate feature engineering for Depresjon dataset from paper \"Comparison of Night, Day and 24 h Motor Activity Data for the Classification of Depressive Episodes\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875d4944",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659cd1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082b9ff1",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ec0ccc",
   "metadata": {},
   "source": [
    "First, we have to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb91083",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./depresjon_data\"\n",
    "condition_dir = os.path.join(data_dir, \"condition\")\n",
    "control_dir = os.path.join(data_dir, \"control\")\n",
    "scores_file = os.path.join(data_dir, \"scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea7c8e",
   "metadata": {},
   "source": [
    "`condition` and `control` directories contain CSV files with measurements, one file per person. For example, `condition_1.csv` contains measurements for patient 1 diagnosed with depression.\n",
    "\n",
    "Those files are read into a list of dataframes, since this makes them easy to process later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73340b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [pd.read_csv(os.path.join(condition_dir, filename)) for filename in os.listdir(condition_dir)]\n",
    "controls = [pd.read_csv(os.path.join(control_dir, filename)) for filename in os.listdir(control_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004b1280",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897a6036",
   "metadata": {},
   "source": [
    "The `scores.csv` file contains static information about patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64366c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_data = pd.read_csv(scores_file)\n",
    "static_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5523a63",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3345db9",
   "metadata": {},
   "source": [
    "We have the data loaded, now we can explore it: check number of measurements, number of columns, their types, missing values etc. First the data for time series will be checked, then for the static data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ea876d",
   "metadata": {},
   "source": [
    "### Number of patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc97c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\"Condition\", \"Control\"]\n",
    "ys = [len(conditions), len(controls)]\n",
    "\n",
    "print(f\"Condition number: {len(conditions)}\")\n",
    "print(f\"Control number: {len(controls)}\")\n",
    "\n",
    "plt.bar(xs, ys)\n",
    "plt.xlabel(\"Group\")\n",
    "plt.ylabel(\"Patients\")\n",
    "plt.title(\"Number of patients per group\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8317f9e8",
   "metadata": {},
   "source": [
    "We definitely have quite heavy imbalance, the control group being about 50% larger than the condition group. This influences many metrics and should be taken into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eaa3bd",
   "metadata": {},
   "source": [
    "### Measurements number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c125f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_rows = pd.Series([len(df) for df in conditions])\n",
    "condition_rows.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbf144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_rows = pd.Series([len(df) for df in controls])\n",
    "control_rows.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2c1ebe",
   "metadata": {},
   "source": [
    "We have greatly varying number of measurements for both condition and control group. In general measurements for control group are longer, but also more varied (both mean and standard deviation are higher). For fair assessment, where each patient gets the same chance to influence the model, the measurements should be of equal length for all cases. However, for calculating aggregations (e.g. mean / median value) this may not be necessary, since they typically reduce the data to a single number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637e20bf",
   "metadata": {},
   "source": [
    "### Columns and types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37f7db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccddfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions[0].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14434777",
   "metadata": {},
   "source": [
    "For measurement files we have 3 columns:\n",
    "- timestamp with precise measurement date and time, with 1 minute measurement resolution\n",
    "- date, information already included in the timestamp\n",
    "- activity, the time series value\n",
    "\n",
    "Data types of columns are definitely wrong - `timestamp` should be a proper timestamp and the `date` column is redundant. This will be fixed in the preprocessing section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e687573",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeb6eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_NaNs = pd.Series([df[\"activity\"].isna().sum() for df in conditions])\n",
    "condition_NaNs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb4b605",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_NaNs = pd.Series([df[\"activity\"].isna().sum() for df in controls])\n",
    "control_NaNs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d11679",
   "metadata": {},
   "source": [
    "We have exactly 0 missing values for the dependent variable, activity. This means that the data was accurately gathered for all patients for the entire duration of measurement period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02fef6a",
   "metadata": {},
   "source": [
    "### Statis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f540a88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c35114",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c7b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in static_data.columns:\n",
    "    print(col, static_data[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b8e06d",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- all data types seem correct\n",
    "- `number` is essentially an index for patients and should not be used as a feature\n",
    "- `days` indicate the number of data collection days, but this information is already included in the time series timestamps\n",
    "- `gender` is correct, but doesn't follow the usual convention of 0-1 values for binary features\n",
    "- `age` should be preprocessed to integers (e.g. `0` for <50 years, `1` for >= 50 years) for classification\n",
    "- `afftype` and `melanch` indicate the clinical state observations for depressed patients and are NaN for non-depressed controls\n",
    "- `edu`, `marriage` and `work` explain socioeconomic status of the patient\n",
    "- `madrs1` and `madrs2` are MADRS score for patients with condition at the beginning and at the end of measurements; they are not used for classification, but they could be used as regression targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa6a930",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d741abf5",
   "metadata": {},
   "source": [
    "In the paper several steps of feature engineering are introduced. They need to be performed, as the typical models for tabular data like Random Forest are used. This approach allows usage of classical ML algorithms on time series data, while also indirectly incorporating time dependencies in form of features derived from the signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e321ea",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b79237",
   "metadata": {},
   "source": [
    "**Warning**: I've found out that the paper is actually inconsistent in what it says and does in the preprocessing. It states that:\n",
    "\"For the pre-processing stage, the next step are proposed. Since the total amount of data recorded for each subject is different, a new subset of data is extracted, adjusting the number of observations to be equal for each subject. Theh, from the new set of data, a segmentation is applied to form one hour data intervals. This segmentation allowed the classification of depressive episodes per hour.\n",
    "\n",
    "Therefore, based on the hourly segmentation, three different subsets are constructed; night motor activity (from 21 to 7 h taking into account the sunrise standard hours) [21], day motor activity (from 8 to 20 h) and finally all day motor activity with the total day hours. The number of observations contained in each dataset is shown in Table 1. After separated the data into day, night and 24 h data were cleaned from missing data.\"\n",
    "\n",
    "According to this, we should:\n",
    "- trim observations to the length of the shortest one, forming \"new set of data\" with same number of observations per subject\n",
    "- segment into 1-hour intervals, calculating average activity\n",
    "- create night dataset, day dataset and dataset with all observations\n",
    "\n",
    "According to the paper, they got the following number of hourly observations:\n",
    "- day: 14168\n",
    "- night: 11945\n",
    "- full data: 26113\n",
    "\n",
    "But this number is wrong. This is approximately the number of raw observations in the dataset, not number of hour segments. In addition, to make sure that night and day data has the same length, it should be trimmed to the same length after splitting. Corrected process:\n",
    "- segment data for each patient into 1-hour intervals, calculating average activity\n",
    "- create night dataset, day dataset and dataset with all observations\n",
    "- trim observations in each dataset to the length of the shortest one\n",
    "\n",
    "This way, we arrive at the following sequences lengths:\n",
    "- day: 9845\n",
    "- night: 7865\n",
    "- full data: 17710"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a93f643",
   "metadata": {},
   "source": [
    "First, correct the data type of `timestamp` column and drop the redundant `date` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d7f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (conditions + controls):\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "for df in (conditions + controls):\n",
    "    if \"date\" in df.columns:\n",
    "        df.drop(\"date\", axis=1, inplace=True)\n",
    "\n",
    "conditions[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a294a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "controls[1].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7f0502",
   "metadata": {},
   "source": [
    "Next, group by hour and calculate mean hourly activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6787c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_hour(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # group by hour, calculate means\n",
    "    timestamps = df[\"timestamp\"]\n",
    "    group_by_cols = [timestamps.dt.year, timestamps.dt.month, timestamps.dt.day, timestamps.dt.hour]\n",
    "    grouped = df.groupby(group_by_cols).mean()\n",
    "\n",
    "    # recreate flat index for 2D table\n",
    "    grouped.index = grouped.index.to_flat_index()\n",
    "    datetime_index_df = pd.DataFrame(grouped.index.values.tolist(), columns=[\"year\", \"month\", \"day\", \"hour\"])\n",
    "    datetime_index = pd.to_datetime(datetime_index_df)\n",
    "\n",
    "    # add back the datetime information\n",
    "    grouped.reset_index(inplace=True)\n",
    "    grouped[\"datetime\"] = datetime_index\n",
    "    grouped.drop(\"index\", axis=1, inplace=True)\n",
    "\n",
    "    # change column order for readability\n",
    "    grouped = grouped.reindex([\"datetime\", \"activity\"], axis=1)\n",
    "    \n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57feacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_preprocessed = [group_by_hour(df) for df in conditions]\n",
    "controls_preprocessed = [group_by_hour(df) for df in controls]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88647480",
   "metadata": {},
   "source": [
    "Lastly, create separate night and day datasets in addition to the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae873262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_night_day_division(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    night_df = df.loc[(df[\"datetime\"].dt.hour >= 21) | (df[\"datetime\"].dt.hour < 8)]\n",
    "    day_df = df.loc[(df[\"datetime\"].dt.hour >= 8) & (df[\"datetime\"].dt.hour < 21)]\n",
    "    return night_df, day_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9c3515",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_night = []\n",
    "conditions_day = []\n",
    "\n",
    "controls_night = []\n",
    "controls_day = []\n",
    "\n",
    "for df in conditions_preprocessed:\n",
    "    night_df, day_df = get_night_day_division(df)\n",
    "    conditions_night.append(night_df)\n",
    "    conditions_day.append(day_df)\n",
    "\n",
    "for df in controls_preprocessed:\n",
    "    night_df, day_df = get_night_day_division(df)\n",
    "    controls_night.append(night_df)\n",
    "    controls_day.append(day_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29564196",
   "metadata": {},
   "source": [
    "Trim to the same number of 1-hour segments, so we have equal length sequences for night, day and all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85be87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length_night = min([len(df) for df in conditions_night + controls_night])\n",
    "min_length_day = min([len(df) for df in conditions_day + controls_day])\n",
    "min_length_all = min(min_length_night, min_length_day)\n",
    "\n",
    "conditions_trimmed = {\n",
    "    \"night\": [df[:min_length_night] for df in conditions_night],\n",
    "    \"day\": [df[:min_length_day] for df in conditions_day],\n",
    "    \"all\": [df[:min_length_all] for df in conditions_night + conditions_day]\n",
    "}\n",
    "\n",
    "controls_trimmed = {\n",
    "    \"night\": [df[:min_length_night] for df in controls_night],\n",
    "    \"day\": [df[:min_length_day] for df in controls_day],\n",
    "    \"all\": [df[:min_length_all] for df in controls_night + controls_day]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cb0718",
   "metadata": {},
   "source": [
    "Let's check the number of hour segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1024f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "night_observations = sum([len(df) for df in conditions_trimmed[\"night\"] + controls_trimmed[\"night\"]])\n",
    "day_observations = sum([len(df) for df in conditions_trimmed[\"day\"] + controls_trimmed[\"day\"]])\n",
    "all_observations= sum([len(df) for df in conditions_trimmed[\"all\"] + controls_trimmed[\"all\"]])\n",
    "\n",
    "print(f\"Day: {day_observations}\")\n",
    "print(f\"Night: {night_observations}\")\n",
    "print(f\"Full data: {day_observations + night_observations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e518e4e6",
   "metadata": {},
   "source": [
    "As you can see, the number of segments is very different than stated in the paper. However, that was probably just a mistake in specifying the table contents and authors got similiar numbers of actual hourly segments as here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166408a1",
   "metadata": {},
   "source": [
    "Before further processing it will come in handy to work on regular Numpy arrays instead of lists of DataFrames. We will have 1 row per patient, with columns indicating measurements (short and wide matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71315ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_night = [df[\"activity\"].values for df in conditions_trimmed[\"night\"]] + \\\n",
    "          [df[\"activity\"].values for df in controls_trimmed[\"night\"]]\n",
    "X_night = np.vstack(X_night)\n",
    "\n",
    "y_night = np.zeros(X_night.shape[0])\n",
    "y_night[:len(conditions_trimmed[\"night\"]) + 1] = 1\n",
    "\n",
    "\n",
    "X_day = [df[\"activity\"].values for df in conditions_trimmed[\"day\"]] + \\\n",
    "        [df[\"activity\"].values for df in controls_trimmed[\"day\"]]\n",
    "X_day = np.vstack(X_day)\n",
    "\n",
    "y_day = np.zeros(X_day.shape[0])\n",
    "y_day[:len(conditions_trimmed[\"day\"]) + 1] = 1\n",
    "\n",
    "\n",
    "X_all = [df[\"activity\"].values for df in conditions_trimmed[\"all\"]] + \\\n",
    "        [df[\"activity\"].values for df in controls_trimmed[\"all\"]]\n",
    "X_all = np.vstack(X_all)\n",
    "\n",
    "y_all = np.zeros(X_all.shape[0])\n",
    "y_all[:len(conditions_trimmed[\"all\"]) + 1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbc18eb",
   "metadata": {},
   "source": [
    "Make sure that the data is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad786920",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_night[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ea2114",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_night"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5e2cf7",
   "metadata": {},
   "source": [
    "There seem to be no missing values, despite what's described in the paper. There is a possibility that dataset available online has already been cleaned in this regard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48106b4",
   "metadata": {},
   "source": [
    "Next we standardize the data, i.e. subtract mean and divide by standard deviation. I assume that standardization is done separately for all 3 sets of data, though this has not been specified in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15683d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_night_standardized = (X_night - X_night.mean()) / X_night.std()\n",
    "X_day_standardized = (X_day - X_day.mean()) / X_day.std()\n",
    "X_all_standardized = (X_all - X_all.mean()) / X_all.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0ec911",
   "metadata": {},
   "source": [
    "### Time domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b7249c",
   "metadata": {},
   "source": [
    "Time features are extracted according to the article:\n",
    "- `mean`, `median`, `stddev`, `variance`, `kurtosis`, `minimum`, `maximum` - quite self explanatory statistical features\n",
    "- `coeff_of_var` - coefficient of variation, the ratio of the biased standard deviation to the mean\n",
    "- `iqr` - interquartile range, difference between 75 and 25 percentile (3rd and 1st quartile)\n",
    "- `trimmed_mean` - alternatively truncated mean, mean of the values where the most extreme values (from both ends) are not used; since the article doesn't specify this, I assume that the popular 10% trim percentage is used\n",
    "\n",
    "Data is saved as a DataFrame, since some machine learning models can provide additional insight when using named columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84ed9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr, kurtosis, trim_mean, variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f05403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(X: np.ndarray) -> pd.DataFrame:\n",
    "    features = {\n",
    "        \"mean\": X.mean(axis=1),\n",
    "        \"median\": np.median(X, axis=1),\n",
    "        \"stddev\": X.std(axis=1),\n",
    "        \"variance\": np.var(X, axis=1),\n",
    "        \"kurtosis\": kurtosis(X, axis=1),\n",
    "        \"coeff_of_var\": variation(X, axis=1),\n",
    "        \"iqr\": iqr(X, axis=1),\n",
    "        \"minimum\": X.min(axis=1),\n",
    "        \"maximum\": X.max(axis=1),\n",
    "        \"trimmed_mean\": trim_mean(X, proportiontocut=0.1, axis=1)\n",
    "    }\n",
    "    return pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930771a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_night_features = extract_features(X_night_standardized)\n",
    "X_day_features = extract_features(X_day_standardized)\n",
    "X_all_features = extract_features(X_all_standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccdd88a",
   "metadata": {},
   "source": [
    "### Frequency domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-stuart",
   "metadata": {},
   "source": [
    "#### First approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b218f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_frames = []\n",
    "\n",
    "for c_i, c in enumerate(conditions, start=1):\n",
    "    c_copy = c.copy()\n",
    "    c_copy['number'] = f'condition_{c_i}'\n",
    "    f_frames.append(c_copy)\n",
    "    \n",
    "for c_i, c in enumerate(controls, start=1):\n",
    "    c_copy = c.copy()\n",
    "    c_copy['number'] = f'control_{c_i}'\n",
    "    f_frames.append(c_copy)\n",
    "\n",
    "f_df = pd.concat(f_frames)\n",
    "\n",
    "f_df['date'] = f_df.apply(lambda r: r['timestamp'].date(), axis=1)\n",
    "f_df['hour'] = f_df.apply(lambda r: r['timestamp'].hour, axis=1)\n",
    "\n",
    "f_df.reset_index()\n",
    "f_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f617d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df.drop('timestamp', axis=1, inplace=True)\n",
    "f_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff4be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df_segments = f_df.groupby(['number', 'date', 'hour'])\n",
    "f_df_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba51e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df_grouped = f_df_segments['activity'].apply(list).to_frame()\n",
    "f_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe3f4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft\n",
    "\n",
    "# Now:\n",
    "# 1. Adjust the number of samples for each 'number'\n",
    "# 2. Create three subsets: night, day, all\n",
    "# 3. Standardize\n",
    "# 4. Do the feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52005b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Frequency-domain feature extraction\n",
    "f_df_grouped['f_activity'] = f_df_grouped.apply(lambda r: fft(r['activity']), axis=1)\n",
    "f_df_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-summer",
   "metadata": {},
   "source": [
    "#### Second approach: hourly_mean  > FFT > PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft\n",
    "from scipy.signal import welch, periodogram\n",
    "from scipy.stats import entropy, moment\n",
    "\n",
    "def as_psd(X: np.ndarray, fs=1):\n",
    "    frequencies, psd_estimates = periodogram(X, fs, axis=-1)\n",
    "    return frequencies, psd_estimates\n",
    "\n",
    "def extract_frequency_features(X: np.ndarray, fs=1) -> pd.DataFrame:\n",
    "    X_fft = fft(X, axis=1)\n",
    "    X_psd_freqs, X_psd = as_psd(X, fs)\n",
    "    X_psd_avgs = np.sum(np.power(np.absolute(X_fft), 2), axis=1) / X_psd_freqs[-1]\n",
    "    \n",
    "    def spectral_flatness(ns):\n",
    "        norm = ns.mean()\n",
    "        norm = 1 if norm == 0 else norm\n",
    "        return np.exp(np.log(ns + 1e-20).mean()) / norm\n",
    "    \n",
    "    features_df = extract_features(X_psd)\n",
    "    features_df['spectral_density'] = X_psd_avgs\n",
    "    features_df['entropy'] = entropy(X_psd, base=2, axis=1) \n",
    "    features_df['skewness'] = moment(X_psd, moment=3, axis=1)\n",
    "    features_df['spectral_flatness'] = np.apply_along_axis(spectral_flatness, arr=X_psd, axis=1)\n",
    "    \n",
    "    features_df = features_df.add_prefix('freq_')\n",
    "    return features_df\n",
    "\n",
    "get_freq_features = lambda X: extract_frequency_features(X, fs=1./3600) # 1 hour sampling frequency\n",
    "\n",
    "X_day_freq_features = get_freq_features(X_day_standardized)\n",
    "X_night_freq_features = get_freq_features(X_night_standardized)\n",
    "X_all_freq_features = get_freq_features(X_all_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_freq_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cf0793",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6bc799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
